# JSON ë°ì´í„° ì²˜ë¦¬ìš© ëª¨ë“ˆ
import json

import ipdb
# íƒ€ìž… ížŒíŠ¸ë¥¼ ìœ„í•œ ëª¨ë“ˆ
from typing import Dict
# ë¬´ìž‘ìœ„ ìˆ˜ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
import random
# tqdm: ë°˜ë³µë¬¸ì˜ ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
from tqdm import trange, tqdm
import ast
# LLM ì¶œë ¥ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
from ..utils.llm_output_check import parse_list
# ë„êµ¬ ë“±ë¡ ë° ì´ˆê¸°í™” í•¨ìˆ˜
from ..base import register_tool, init_tool_instance
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
from ..prompts_en2 import question_asker_system, expert_system, \
    dlg_based_writer_system, dlg_based_writer_prompt, chapter_writer_system

from mm_story_agent.base import register_tool
from mm_story_agent.modality_agents.LLMqwen import QwenAgent  # QwenAgent ë¶ˆëŸ¬ì˜¤ê¸°
from mm_story_agent.modality_agents.LLMexaone import ExaoneAgent  # ExaoneAgent ë¶ˆëŸ¬ì˜¤ê¸°

#ì „ì²´ ì´ì•¼ê¸° ìƒì„± íŒŒì´í”„ ë¼ì¸ì˜ ì¤‘ì‹¬ í´ëž˜ìŠ¤ MMStoryAgentë¥¼ ì •ì˜í•˜ê³  ì‹¤í–‰í•˜ëŠ” ê²ƒ
from mm_story_agent.prompts_en2 import (
    scene_expert_system,
    scene_amateur_questioner_system,
    scene_refined_output_system,
)

# JSON í˜•íƒœì˜ outlineì´ ìœ íš¨í•œì§€ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
def json_parse_outline(outline):
    # ì½”ë“œ ë¸”ë¡ ë§ˆí¬ë‹¤ìš´(```json ë“±)ì„ ì œê±°
    outline = outline.strip("```json").strip("```")
    try:
        # ë¬¸ìžì—´ì„ JSONìœ¼ë¡œ íŒŒì‹±
        outline = json.loads(outline)
        # ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        if not isinstance(outline, dict):
            return False
        # í•„ìš”í•œ í‚¤ë§Œ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸
        if outline.keys() != {"story_title", "story_outline"}:
            return False
        # ê° ì±•í„°ê°€ í•„ìš”í•œ í‚¤ë¥¼ ê°€ì§€ê³  ìžˆëŠ”ì§€ í™•ì¸
        for chapter in outline["story_outline"]:
            if chapter.keys() != {"chapter_title", "chapter_summary"}:
                return False
    except json.decoder.JSONDecodeError:
        # JSON íŒŒì‹± ì‹¤íŒ¨
        return False
    return True  # ëª¨ë“  ì¡°ê±´ í†µê³¼ ì‹œ True ë°˜í™˜

# ì§ˆë¬¸ ìƒì„± LLM ê³¼ ì „ë¬¸ê°€ ì—­í•  LLMì„ í†µí•´ Q & A ëŒ€í™” ìƒì„± 
@register_tool("qa_outline_story_writer")
class QAOutlineStoryWriter:


    # í´ëž˜ìŠ¤ ìƒì„±ìž
    def __init__(self, cfg: Dict):
        self.cfg = cfg
        # LLM ìƒì„± ì˜¨ë„ ì„¤ì • (ê¸°ë³¸ê°’ 1.0)
        self.temperature = cfg.get("temperature", 1.0)
        # ëŒ€í™” ìµœëŒ€ í„´ ìˆ˜ ì„¤ì •
        self.max_conv_turns = cfg.get("max_conv_turns", 3)
        # ìƒì„±í•  ì•„ì›ƒë¼ì¸ ê°œìˆ˜ ì„¤ì •
        self.num_outline = cfg.get("num_outline", 4)
        # ì‚¬ìš©í•  LLM ì¢…ë¥˜ ì„¤ì •
        self.llm_type = cfg.get("llm", "qwen")

    # ëŒ€í™” ê¸°ë°˜ ì•„ì›ƒë¼ì¸ì„ JSON í˜•íƒœë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    def generate_outline(self, params):
        # params: ì´ì•¼ê¸° ì„¤ì • ì •ë³´ (ì˜ˆ: ì œëª©, ì£¼ì¸ê³µ ë“±)

        # ì§ˆë¬¸ ìƒì„±ìš© LLM ì´ˆê¸°í™”
        asker = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": question_asker_system,
                "track_history": False
            }
        })

        # ì „ë¬¸ê°€ ì—­í•  LLM ì´ˆê¸°í™”
        expert = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": expert_system,
                "track_history": False
            }
        })

        # ì§ˆë¬¸-ë‹µë³€ ê¸°ë¡ ì €ìž¥ ë¦¬ìŠ¤íŠ¸
        dialogue = []
        for turn in trange(self.max_conv_turns):  # ìµœëŒ€ ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ ë°˜ë³µ
            dialogue_history = "\n".join(dialogue)  # í˜„ìž¬ê¹Œì§€ ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê²°í•©

            # ë³€ê²½
            full_context = params.get("full_context", str(params))
            # ì§ˆë¬¸ ìƒì„±
            question, success = asker.call(
                f"Story setting: {full_context}\nDialogue history: \n{dialogue_history}\n",
                temperature=self.temperature
            )
            question = question.strip()

            # 'ëŒ€í™” ì¢…ë£Œ' ë¬¸êµ¬ê°€ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
            if question == "Thank you for your help!":
                break

            # ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            dialogue.append(f"You: {question}")

            # ì „ë¬¸ê°€ì˜ ë‹µë³€ ìƒì„±
            answer, success = expert.call(
                f"Story setting: {full_context}\nQuestion: \n{question}\nAnswer: ",
                temperature=self.temperature
            )
            answer = answer.strip()
            dialogue.append(f"Expert: {answer}")

        # ì•„ì›ƒë¼ì¸ ìž‘ì„±ê¸° LLM ì´ˆê¸°í™”
        writer = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": dlg_based_writer_system,
                "track_history": False
            }
        })
        # full_context ê°€ì ¸ì˜¤ê¸°
        full_context = params.get("full_context", "")
        # ì•„ì›ƒë¼ì¸ ìž‘ì„±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        writer_prompt = dlg_based_writer_prompt.format(
            story_setting=full_context,
            dialogue_history="\n".join(dialogue),
            num_outline=self.num_outline
        )

        # ì•„ì›ƒë¼ì¸ ìƒì„± ì‹œë„
        outline, success = writer.call(writer_prompt, success_check_fn=json_parse_outline)

        # ë””ë²„ê¹…ìš© ì¶œë ¥
        try:
            preview_outline = outline.replace("```json", "").replace("```", "").strip()
            parsed_outline = json.loads(preview_outline)
            print("ðŸ” [DEBUG] Parsed Outline:")
            print(json.dumps(parsed_outline, ensure_ascii=False, indent=4))
        except Exception as e:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì¶œë ¥:")
            print(repr(outline))
        # ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼ì¸ ê²½ìš° ì—ëŸ¬ ë°œìƒ
        if not outline or not outline.strip():
            raise ValueError(f"LLMì—ì„œ ìœ íš¨í•œ outlineì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {repr(outline)}")

        # ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡ ì œê±°
        outline = outline.replace("```json", "").replace("```", "").strip()

        # JSON íŒŒì‹±
        outline = json.loads(outline)

        # ìµœì¢… outline ë°˜í™˜
        return outline

    # ì•„ì›ƒë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì±•í„°ë³„ ìƒì„¸ ì´ì•¼ê¸° ìƒì„±
    def generate_story_from_outline(self, outline):
        # ì±•í„° ìž‘ì„±ìš© LLM ì´ˆê¸°í™”
        chapter_writer = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": chapter_writer_system,
                "track_history": False
            }
        })

        all_pages = []  # ì „ì²´ ì´ì•¼ê¸° íŽ˜ì´ì§€ ë¦¬ìŠ¤íŠ¸
        for idx, chapter in enumerate(tqdm(outline["story_outline"])):
            # í˜„ìž¬ê¹Œì§€ì˜ ë‚´ìš©ê³¼ ìƒˆë¡œìš´ ì±•í„° ì •ë³´ë¥¼ ìž…ë ¥ìœ¼ë¡œ ì „ë‹¬
            chapter_detail, success = chapter_writer.call(
                json.dumps(
                    {
                        "completed_story": all_pages,
                        "current_chapter": chapter
                    },
                    ensure_ascii=False
                ),
                success_check_fn=parse_list,  # ì¶œë ¥ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸
                temperature=self.temperature
            )

            # ì‹¤íŒ¨í•œ ê²½ìš° ì‹œë“œ ëžœë¤ê°’ ì£¼ì–´ ìž¬ì‹œë„
            while success is False:
                chapter_detail, success = chapter_writer.call(
                    json.dumps(
                        {
                            "completed_story": all_pages,
                            "current_chapter": chapter
                        },
                        ensure_ascii=False
                    ),
                    seed=random.randint(0, 100000),  # ì‹œë“œ ë³€ê²½
                    temperature=self.temperature,
                    success_check_fn=parse_list
                )

            # ë¬¸ìžì—´ë¡œ ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹± (eval ì‚¬ìš©)
            pages = [page.strip() for page in eval(chapter_detail)]
            all_pages.extend(pages)  # ì „ì²´ íŽ˜ì´ì§€ì— ì¶”ê°€

        # ëª¨ë“  íŽ˜ì´ì§€ ë°˜í™˜
        return all_pages

    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    def call(self, params):
        # 1ë‹¨ê³„: ì•„ì›ƒë¼ì¸ ìƒì„±
        outline = self.generate_outline(params)
        # 2ë‹¨ê³„: ì•„ì›ƒë¼ì¸ ê¸°ë°˜ ì±•í„°ë³„ ì´ì•¼ê¸° ìƒì„±
        pages = self.generate_story_from_outline(outline)
        # ìµœì¢… íŽ˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return pages
    

    # í…ìŠ¤íŠ¸ ì •ì œ ë° êµì •í•˜ëŠ” ì—­í•  

# @register_tool : ê° ì—ì´ì „íŠ¸ì— ì´ë¦„ì„ ë¶™ì—¬ì„œ base.py ì— ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡

# ì •ì œ ì—ì´ì „íŠ¸
@register_tool("RefineWriterAgent")
class RefineWriterAgent:
    # config ì„¤ì • ì •ë³´ ë°›ì•„ì™€ì„œ ì´ˆê¸°í™” ë° LLM.pyì˜ ëª¨ë¸ì„ ê°€ì ¸ì™€ ì´ˆê¸°í™”
    def __init__(self, cfg):
        self.llm = ExaoneAgent(cfg)

    # ìž…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë”•ì…”ë„ˆë¦¬ ë°›ê³  "raw_text"ë¼ëŠ” í‚¤ë¥¼ í†µí•´ ì •ì œí•  ì›ë¬¸ ë°›ê¸°
    def call(self, params):
        print("[RefineWriterAgent] ì „ì²´ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘")  
        prompt = params["raw_text"]
        response, _ = self.llm.call(prompt)
        print("[RefineWriterAgent] ì „ì²´ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ.")  
        return response

# ì „ì²´ ì´ì•¼ê¸°ë¥¼ ìž¥ë©´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
# ìž…ë ¥ : full_text / ì²˜ë¦¬ : LLMìœ¼ë¡œ ìž¥ë©´ì„ ì¶”ì¶œ
# ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def parse_list(output: str):
    try:
        parsed = ast.literal_eval(output)
        return isinstance(parsed, list)
    except Exception:
        return False

from tqdm import trange
import time  # optional: sleep() ë„£ê³  ì‹¶ë‹¤ë©´ ì‚¬ìš©

@register_tool("SceneExtractorAgent")
class SceneExtractorAgent:
    def __init__(self, cfg):
        self.cfg = cfg
        self.temperature = cfg.get("temperature", 0.7)
        self.max_conv_turns = cfg.get("max_conv_turns", 3)
        self.llm_type = cfg.get("llm", "qwen")

        # ê° ì—­í• ì˜ LLM ì´ˆê¸°í™”
        print("[INFO] ì „ë¬¸ê°€ LLM ì´ˆê¸°í™”")
        self.expert = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": scene_expert_system,
                "track_history": False
            }
        })

        print("[INFO] ì•„ë§ˆì¶”ì–´ LLM ì´ˆê¸°í™”")
        self.amateur = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": scene_amateur_questioner_system,
                "track_history": False
            }
        })

        print("[INFO] ì •ì œ LLM ì´ˆê¸°í™”")
        self.refiner = init_tool_instance({
            "tool": self.llm_type,
            "cfg": {
                "system_prompt": scene_refined_output_system,
                "track_history": False
            }
        })

    def call(self, params):
        full_text = params["full_text"]
        dialogue = []

        print("\n[STEP 1] Generating initial scene draft (Expert)...")
        initial_scene, _ = self.expert.call(full_text, temperature=self.temperature)
        print(">>> Initial Scene Draft:\n", initial_scene.strip(), "\n")
        dialogue.append(f"Expert: {initial_scene.strip()}")

        print("[STEP 2] Starting Q&A Refinement Loop...\n")
        for turn in trange(self.max_conv_turns, desc="Q&A Turns"):
            print(f"\n--- Turn {turn+1} ---")
            history = "\n".join(dialogue)

            question, _ = self.amateur.call(f"{full_text}\n{history}", temperature=self.temperature)
            print(f"[Amateur's Question]: {question.strip()}")
            dialogue.append(f"Amateur: {question.strip()}")

            answer, _ = self.expert.call(f"{full_text}\nQuestion: {question.strip()}", temperature=self.temperature)
            print(f"[Expert's Answer]: {answer.strip()}")
            dialogue.append(f"Expert: {answer.strip()}")

        print("\n[STEP 3] Refining final scene list...")
        final_prompt = "\n".join(dialogue)
        final_scene_list, success = self.refiner.call(
            f"{full_text}\n{final_prompt}",
            success_check_fn=parse_list,
            temperature=self.temperature
        )

        if not success:
            print("[ERROR] Scene extraction failed.")
            raise ValueError("Scene extraction failed.")

        print("\nâœ… Scene extraction complete. Final scene list:")
        ipdb.set_trace()
        print(final_scene_list)
        return eval(final_scene_list)

@register_tool("SummaryWriterAgent")
class SummaryWriterAgent:
    def __init__(self, cfg):
        self.llm = QwenAgent(cfg)
    def call(self, params):
        joined = "\n".join(params["scene_text"])
        response, _ = self.llm.call(f"Summarize the story: {joined}")
        return response

@register_tool("MetaWriterAgent")
class MetaWriterAgent:
    def __init__(self, cfg: Dict):
        self.cfg = cfg
        self.temperature = cfg.get("temperature", 1.0)
        self.qwenllm = cfg.get("llm", "qwen")
        self.exaonellm = cfg.get("llm", "exaone")
    def generateMetaData(self, params):
        pass